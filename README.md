Лабораторные работы по NLP (компьютерная лингвистика):
1. [Парсинг](news_collection.ipynb) новостного сайта (bbc.com) для получения XML-файла с текстом статьи названием и ее тегом.
2. [Поиск по тексту](search+tfidf.ipynb). Реализация наивного поиска (каждое слово поискового запроса есть в статье или хотя бы одно) и td-idf поиск (на основе близости tf-idf векторов запроса и статьи).
3. [Классификация](classification.ipynb) ранее полученных текстов (их TF-IDF представления) по категориям c помощью Logistic Regression, SVC, SVM, Random Forest, Gradient Boosting Trees, Naive Bayes, Ada Boost, kNN, Perceptron. Сравнение результатов алгоритмов и анализ ошибок. Спойлер: по точности выигрывают SVC и 3-слойный перцептрон, но первому требуется меньше секунды на предсказание, а нейронной сети - а то и полминуты.
4. [Теггер](tagging.ipynb) для определения части речи слова на основе юниграмм, двуграмм, триграмм.
5. [LDA](LDA_topics.ipynb) - 1 часть - кластеризация текстов и выделение ключевых слов для определения, к какой категории можно отнести блок текстов. 2 часть - классификация текстов с помощью Word2Vec представления.
6. [Богохульная генерация текстов](text_generation.ipynb) с помощью модели, обученной на Библии, построенной на Keras.Серия экспериментов с разными optimizer и LR для получения лучшей точности + рыжий кот в конце. Моя любимейшая лабораторная.
